---
title: "Visualization of regression models"
output:
  # pdf_document:
  #   toc: yes
  # word_document:
  #   toc: yes
  html_document:
    toc: yes
    toc_float: yes
    keep_md: yes #renders on GitHub but with an ugly YAML table on top
    # github_document:
    # toc: true #does not render floating toc
---

```{r options, include=FALSE}
#this chunk itself will be hidden from the notebook though since include = FALSE.

knitr::opts_chunk$set(
	echo = TRUE, #show output
	message = FALSE, #don't show messages
	warning = FALSE #don't show warnings
)
```

Two-part tutorial ([part 1](https://www.youtube.com/watch?v=BNTn_f43U04), [part 2](https://www.youtube.com/watch?v=wev5a3rwsvo)) by Yury Zablotski on how to visualize a wide variety of regression models, including linear, polynomial, logistic, multinomial, mixed models.

He works on the `Wage` data set that comes with the ISLR package. Only 1000 observations from the `Wage` package are sampled.
```{r setup}
library(tidyverse)
library(broom)
library(gt)
library(ISLR)

set.seed(1)

d <- Wage %>% 
  sample_n(1000) %>% 
  rename(salary = wage)
```

# 1. Simple linear model with one categorical predictor
1. `effects::allEffects()` computes the mean values for each level of the categorical predictor
2. `lm()` computes mean values for each level of a predictor relative to its first level
3. enclosing`allEffects()` within `base::plot()` allows plotting of the mean of the various levels of the predictor
```{r cat1}
# 1. simple linear model with one categorical predictor
cat1_m <- lm(salary ~ jobclass, d)

library(effects)

#`allEffects()` computes the mean at each level of the categorical predictor
# `lm()` computes the mean at each level of a predictor relative to its first level
cat1_m %>% allEffects() %>% names()
cat1_m %>% allEffects() %>% pluck("jobclass") %>% as_tibble() %>% gt()
cat1_m %>% tidy() %>% gt()

plot(allEffects(cat1_m)) 
```

# 2. Simple linear model with one numeric predictor
1. `allEffects()` derives fitted-y values against multiple equidistant values ("pseudo-levels") of the numeric predictor, thus plotting the effect of the predictor at multiple levels 
2. `lm()` estimates the slope of the whole line (i.e. it estimates model coefficient not fitted values)
```{r num1}
# 2. simple linear model with one numeric predictor
num1_m <- lm(salary ~ age, d)

#allEffects() derives fitted-y values against multiple equidistant values ("pseudo-levels") of the numeric predictor, thus plotting the effect of the predictor at multiple levels 
# `lm()` estimates the slope of the whole line (i.e. it estimates model coefficient not fitted values)
num1_m %>% allEffects() %>% names()
num1_m %>% allEffects() %>% pluck("age") %>% as_tibble() %>% gt()
num1_m %>% tidy() %>% gt()

plot(allEffects(num1_m))
# plot(allEffects(num1_m), grid = TRUE) # overlays a grid on the plot
```

# 3. Multiple linear model with two categorical predictors
```{r cat2_1}
# 3. multiple linear model with two categorical predictors
cat2_m <- lm(salary ~ jobclass + education, d)

#makes two plots: one for each categorical predictor
plot(allEffects(cat2_m)) 
#makes one plot: for the specified predictor
plot(predictorEffect(predictor = "education", mod = cat2_m))
```

## Only the way `allEffects()` & `lm()` display their results differs, the underlying estimates (coefficients) are same {#cat2.1}
1. with `lm()` the output is NOT displayed in terms of estimates (coefficients), rather each factor gets a separate baseline and the values of all other levels within a factor are displayed relative to the within-factor baseline (i.e. against the respective first level). Thus, `lm()` puts out effects sizes rather than coefficients.
2. with 0 or -1 in the formula, `lm()` does not give the first factor a baseline, i.e. the values of its levels are displayed relative to zero; however, the second factor still gets a baseline, i.e. its values are displayed relative to its first level.
3. with `allEffects()`, neither of the factors get a baseline and the values of all levels within a factor are displayed relative to zero
4. irrespective of the displayed values, all the effects (i.e. pairwise difference relative to baseline) are identical for `lm()` and `allEffects()`. Only the estimate (i.e. the coefficient) of the first level of the first factor is different.
5. to calculate the estimates (coefficients) from the `lm()` output just add the relevant values together
summary: `lm()` and `allEffects()` produce the same pairwise effects within all factors (i.e. differences from the respective baselines), but the estimate of first level of the first factor is different

```{r cat2_2}
#proof that estimate of the first level of the first factor differs between `lm()` and `allEffects()`
f1l1_allEffects <- cat2_m %>% allEffects() %>% pluck("education") %>% as_tibble() %>% filter(education == '1. < HS Grad') %>%  select(fit)

f1l1_lm <- cat2_m %>% tidy() %>% filter(term == '(Intercept)') %>%  select(estimate)

isTRUE(f1l1_allEffects != f1l1_lm)
```

## The effects values produced by the additive and the standalone models differ
1. the value of `jobclass` when education is present in the model is different from its value when education is absent
2. the value of education when `jobclass` is present in the model is different from its value when `jobclass` is absent

```{r cat2_3}
lm(salary ~ jobclass, d) %>% allEffects() %>% pluck("jobclass")  %>% as_tibble() %>% gt() %>% tab_header(data = ., title = "job_alone_allEffects")

lm(salary ~ jobclass + education, d) %>% allEffects() %>% pluck("jobclass")  %>% as_tibble() %>% gt() %>% tab_header(data = ., title = "job_additive_allEffects")

lm(salary ~ education, d) %>% allEffects() %>% pluck("education") %>% as_tibble() %>% gt() %>% tab_header(data = ., title = "edu_alone_allEffects")

lm(salary ~ jobclass + education, d) %>% allEffects() %>% pluck("education")  %>% as_tibble() %>% gt() %>% tab_header(data = ., title = "edu_additive_allEffects")
```

3. apart from the estimate of factor 1, level 1 in the additive model, `lm()` produces effects identical to the corresponding `allEffects()` models. Observe that effects in `alone_lm` models correspond to the effects in `alone_allEffects` models. Similarly, `additive_lm` effects correspond to `additive_allEffects` effects.
```{r cat2_4}
lm(salary ~ jobclass, d) %>% tidy() %>% gt() %>% tab_header(data = ., title = "job_alone_lm")

lm(salary ~ education, d) %>% tidy() %>% gt() %>% tab_header(data = ., title = "edu_alone_lm")

lm(salary ~ jobclass + education, d) %>% tidy() %>% gt() %>% tab_header(data = ., title = "additive_lm")
```

# 4. Multiple linear model with two numeric predictors
1. the effect of `age` when `year` is present in the model is different from its effect when `year` is absent

```{r num2_1}
# 4. multiple linear model with two numeric predictors
num2_m <- lm(salary ~ age + year, d)

# # the effect of age when year is present in the model is different from its effect when year is absent
# num2_m %>% allEffects() %>% pluck("age") %>% as_tibble() %>% gt()
# num2_m %>% lm() %>% gt()


```

2. `allEffects()` can add bar-type CIs around fitted-y values corresponding to different "pseudo-levels" of the numeric predictor. This allows comparisons of the effects at the different "pseudo-levels" of the numeric predictor.

```{r num2_2}
# makes two plots one for each numerical predictor, with CI as a ribbon around the regression line
plot(allEffects(num2_m)) 
# `allEffects()` can add bar-type CIs around fitted-y values corresponding to different "pseudo-levels" of the numeric predictor. This allows comparisons of the effects at the different "pseudo-levels" of the numeric predictor.
plot(allEffects(num2_m), confint=list(style='bars'))
```

3. `sjPlot::plot_model()` plots the coefficients, while `effects::allEffects()` displays model predictions (fitted values)
```{r num_2_3}
#3. `sjPlot::plot_model()` plots the coefficients, while `effects::allEffects()` displays model predictions (fitted values)

library(sjPlot)
plot_model(num2_m)
```


# 5. Multiple linear model with one categorical and one numeric predictor {#multiple_plots}

1. the plot() args can be modified to make the plots less cramped 
```{r cat_num_1}
# 5. multiple linear model with one categorical and one numeric predictor
cat_num_m <- lm(salary ~ age + education, d)

# the plot() args can be modified to make the plots less cramped
plot(allEffects(cat_num_m))
plot(allEffects(cat_num_m), rows = 2, cols = 1)
```

2. `plot_model()` arguments can be tweaked to display coefficient values and their significance
```{r cat_num_2}
# `sjPlot::plot_model()` plots the coefficients, while `effects::allEffects()` displays model predictions (fitted values)
# plot_model(cat_num_m)
plot_model(cat_num_m, show.values = T)
```

# Bonus 1: check all the model assumptions and performance in one multi-plot figure
1. the VIF (y-axis in the multicollinearity plot) should be below 5 for factors and below 10 for interactions 

```{r bonus_1}
# bonus 1: check all the model assumptions and performance in one multi-plot figure; 
# alternative to base::plot(model_name)
library(performance)
check_model(cat_num_m)
# the VIF (y-axis in the multicollinearity plot) should be below 5 for factors and below 10 for interactions 
```

# 6. Multiple linear model with interactions
## Only the way `allEffects()` & `lm()` display their results differs, the underlying estimates (coefficients) are same
1. irrespective of the displayed values, all the effects (i.e. pairwise difference relative to baseline) are identical for `lm()` and `allEffects()`. Even the estimate (i.e. the coefficient) of the first level of the first factor is different. Compare with the point 4 of the [notes on categorical models]{#cat2.1} 

```{r inter_1}
# 6. multiple linear model with interactions
inter_m <- lm(salary ~ jobclass * education, d)

## only the way `allEffects()` & `lm()` display their results differs, the underlying estimates (coefficients) are same
#####
# * with `lm()` the output is NOT displayed in terms of estimates (coefficients), rather each factor gets a separate baseline and the values of all other levels within a factor are displayed relative to the within-factor baseline (i.e. against the respective first level). Thus, `lm()` puts out effects sizes rather than coefficients.
# * with 0 or -1 in the formula, `lm()` does not give the first factor a baseline, i.e. the values of its levels are displayed relative to zero; however, the second factor still gets a baseline, i.e. its values are displayed relative to its first level.
# * with `allEffects()`, neither of the factors get a baseline and the values of all levels within a factor are displayed relative to zero
# * irrespective of the displayed values, all the effects (i.e. pairwise difference relative to baseline) are identical for `lm()` and `allEffects()`. Even the estimate (i.e. the coefficient) of the first level of the first factor is identical (compare with the corresponding point made about additive models).
# * to calculate the estimates (coefficients) from the `lm()` output just add the relevant values together
# summary: `lm()` and `allEffects()` produce the same pairwise effects within all factors (i.e. differences from the respective baselines), but the estimate of first level of the first factor is different
inter_m %>% allEffects() %>% pluck("jobclass:education") %>% as_tibble() %>% gt() %>% tab_header(data = ., title = "job*education_allEffects")
inter_m %>% tidy() %>% gt() %>% tab_header(data = ., title = "job*education_lm")

```

2. `allEffects()` recognizes when a model contains an interaction: it produces conjoined subplots (facets); compare with [output of the additive model]{#multiple_plots}, where the plot was not conjoined (not faceted)
```{r inter_2}

# `allEffects()` recognizes when a model contains an interaction: it produces conjoined subplots (facets); 
# compare with output of the additive model, where the plot was not conjoined (not faceted)
plot(allEffects(inter_m))
```

3. `allEffects()` can also be used to produce conventional interaction plots where multiple predictors are displayed in the same plot rather than in facets and their respective levels are joined by a line 
```{r inter_3}
# includes the lines for both the variables in the same facet rather than in conjoined facets
plot(allEffects(inter_m), lines = list(multiline = T))
# remakes the above plot with CIs
plot(allEffects(inter_m),
     lines = list(multiline = T),
     confint = list(style = "auto"))
#an alternative to above
plot_model(inter_m, type = "int")
```

# Bonus 2: post-hoc comparisons
1. We use the `emmeans` package for this. Marginal means are also called least-squares means.
2. By tweaking the formula inside `emmeans()`, diverse comparisons, both between & along the lines, become possible
3. Multiple comparisons are made by controlling the false discovery rate (aka Benjamini-Hochberg correction)
```{r post-hoc}
library(emmeans)
#compare job classes within every education level: between lines
emmeans(inter_m, pairwise ~ jobclass | education, adjust = "fdr")$contrasts %>% tidy() %>% gt()

#compare education levels within a job class: along lines
emmeans(inter_m, pairwise ~ education | jobclass, adjust = "fdr")$contrasts %>% tidy() %>% gt()

#compare every category to every other category
emmeans(inter_m, pairwise ~ education * jobclass, adjust = "fdr")$contrasts %>% tidy() %>% gt()
```

# 7. Multiple linear model with interaction between one categorical and one numeric predictor
1. Setting `confint = list(style = "auto")` gives smooth CIs across the numeric predictors
2. `plot_model()` can also be used to make interaction plots. Its advantage over the `allEffects()` option is that it follows `ggplot2` synta and therefore layers can be added to it.
3. `check_model()` reveals that `health` is correlated with `age:health`, which should be expected since it is a part of the interaction. However, any correlation below 20 is acceptable.
```{r cat_num_inter}
# 7. multiple linear model with interaction between one categorical and one numeric predictor
cat_num_inter <- lm(salary ~ age * health, d)

plot(
  allEffects(cat_num_inter),
  lines = list(multiline = T),
  confint = list(style = "auto"))

plot_model(cat_num_inter, type = "int") + 
  theme_classic() +
  theme(legend.position = "top") +
  xlab("something different")

check_model(cat_num_inter)
```

# Bonus 3: make quick non-linear models with ggplot::geom_smooth()
```{r ggplot_gam}
ggplot(d, aes(age, salary)) +
  geom_point() +
  geom_smooth() + # the quickest way to buld a GAM for numeric data
  facet_grid(jobclass~health) # quick multiple model visualization
```

# 8. Multiple non-linear polynomial model with interactions
1. We logarithmize `salary`, use polynomial function of `age`, and check for interaction with `health. This helps to reduce VIF between the interaction terms as well as to normalize the residuals as revealed by `check_model()`
2. As usual `allEffects()` can be used to plot the model; the `multiline` argument shows the effects of both the first and second degrees of the polynomial on the same plot.

```{r polynomial}
polynomial_m <- lm(log(salary) ~ poly(age, 2) * health, d)

plot(
  allEffects(polynomial_m),
  lines = list(multiline = T),
  confint = list(style = "auto")
)

plot_model(polynomial_m, show.values = T)

check_model(polynomial_m)
```

# 9. Multiple non-linear Generalized Additive Models (GAMs)
1. A GAM cuts the x-axis into several pieces and plots a simple linear model to every piece and then it connects the pieces. `ggplot::geom_smooth()` also does this.
2. The GAM recapitulates the recession of 2007-2008 when plotting the effect of `year`. A simple linear model would miss this.

```{r GAM, eval=FALSE, include=FALSE}
library(gam)

# mgcv is another package for building GAMs, to avoid a conflict specify the package explicitly
# `s()` indicates that we would like a smoothing spline
gam_m <- gam::gam(salary ~ s(year, df = 4) + s(age, df = 5) + education + jobclass, data = d)
summary(gam_m)

par(mfrow = c(2,2))
plot.Gam(gam_m, se = TRUE, col = "blue")

```

# 10. mulitple logisitc regression with interactions
The model measures the probability of being in good health depending on job class and health insurance. All three `jobclass` and `health_ins` and `health` are categorical variables. Since `health` is also binomial, the model outcome is specified by setting the `family = binomial`.
```{r logistic}
logistic_m <- glm(health ~ jobclass * health_ins, d, family = binomial)

plot(allEffects(logistic_m))

plot_model(logistic_m, type = "int")

emmeans(logistic_m, pairwise ~ jobclass | health_ins, adjust = "fdr")$contrasts
emmeans(logistic_m, pairwise ~  health_ins | jobclass, adjust = "fdr")$contrasts
emmeans(logistic_m, pairwise ~  health_ins * jobclass, adjust = "fdr")$contrasts
```

# Bonus 4: visualize the post-hoc analysis with the Pairwise P-value Plot
```{r pwpp}
pwpp(emmeans(logistic_m, ~  health_ins * jobclass), type = "response", adjust = "fdr") + theme_minimal()
```

# 11. Multinomial logistic regression models via neural networks
For categorical response variables with more than two categories we perform multinomial regression.
```{r multinomial}
d <- foreign::read.dta("https://stats.idre.ucla.edu/stat/data/hsbdemo.dta")

multinomial_m <- nnet::multinom(prog ~ ses + write, d)

# for a predictor, the curve of each response category can be plotted separately or in the same plot
plot(allEffects(multinomial_m))

plot(allEffects(multinomial_m),
  lines = list(multiline = T),
  confint = list(style = "auto"))
```

# 12. Multiple Linear Mixed-Effects-Model with interactions
```{r lmer}
library(lme4)
library(lmerTest)
set.seed(9)
d <- InstEval %>% 
  group_by(service, studage) %>% 
  sample_n(100) %>% 
  mutate(dept = as.numeric(dept))

mixed_m <- lmer(y ~service * studage + (1|s) + (1|d), data = d)
linear_m <- lm(y ~service * studage, data = d) #for ANOVA in Bonus 5


plot(allEffects(mixed_m))

plot(
  allEffects(mixed_m),
  lines = list(multiline = T),
  confint = list(style = "auto"))

# post-hocs
emmeans(mixed_m, pairwise ~ service | studage, adjust = "none")$contrasts
pwpp(emmeans(mixed_m, ~ service * studage), type = "response", adjust = "none") + theme_minimal()
```

# Bonus 5: how to choose the best model
Compare AIC & BIC obtained from between-model ANOVA: lower is better.
```{r model_comparison}
anova(mixed_m, linear_m)
```

# 13. GAMMs - Multiple Generalised Additive Mixed Effects Models
A GAM with random effects in it
```{r GAMM, eval=FALSE}
library(mgcViz)

gamm_m <- gammV(y ~ s(as.numeric(d), k = 3), random = list(s = 1), data = InstEval %>% slice(1:5000)) 

plot(gamm_m, allTerms = T)
```

# 14. Kaplan-Meir survival model
Survival models have two response variables: time and event
```{r survival, eval=FALSE}
# install.packages("survival")
# install.packages("survminer")
library("survival")
library("survminer")

set.seed(1)
d <- lung %>% 
  filter(ph.ecog != 3) %>% 
  sample_n(100)

survival_m <- survfit(Surv(time, status) ~ ph.ecog, data = d)

ggsuvplot(survival_m)

# tests for difference in the median survival times among all curves (categories of predictor); but does not tell you which pair of curves are different
ggsuvplot(survival_m,
          pval = TRUE,
          risk.table = "abs_pct",
          surv.median.line = "hv")

#post-hoc test for survival analysis
pairwise_survdiff(
  formula = Surv(time, status) ~ ph.ecog, data = d, p.adjust.method = "fdr"
)
```

# 15. Exponential Parametric Models
Unlike Kapaln-Meir models, these fit smooth exponential (thus parametirc) curves to the survival data so that you can make predictions about survival at different time points. This cannot be done with the Kaplan-Meir curves due to their step-wise nature (the steps assume that if since nobody dies between two events the probability of survival does not change--which is unrealistic).
```{r survival_exp, eval=FALSE}
library(flexsurv) # for parametric survival modelling

survival_exp_m <- flexsurvreg(Surv(time, status) ~ factor(ph.ecog), data = d, dist = "exponential")

ggsurvplot(survival_exp_m)
```

# 16. Cox proportional hazard models
Unlike Kapaln-Meir models, Cox models allow including multiple predictors in the model
```{r cox, eval=FALSE}
cox_m <- coxph(Surv(time, status) ~ age + sex + ph.ecog, data = d)

#get a forest plot ranking the different predictors along with their effect sizes
#It also shows the global p-value for the model and its AIC
ggforest(cox_m, data = d)
```

